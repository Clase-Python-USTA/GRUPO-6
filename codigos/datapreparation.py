# -*- coding: utf-8 -*-
"""DataPreparation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sFYlDbIFDa7ckZ0PnXCMyjF0udcUuhC9
"""

# ==== CELDA 1: SETUP ====
!pip -q install openpyxl pyarrow

import os, re, json, unicodedata, warnings
from datetime import datetime
from typing import Dict, Any, List, Optional
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings("ignore")
pd.set_option("display.max_columns", None)

print("Entorno listo ‚úÖ")

# ==== CELDA 2: CONFIGURA TU RUTA DIRECTA ====
# Si est√° en Drive:
# from google.colab import drive; drive.mount('/content/drive')
# INPUT_PATH = "/content/drive/MyDrive/tu_carpeta/base_datos_completa_NNA_TI_anon.xlsx"

# üëâ EDITA ESTA L√çNEA CON TU RUTA REAL:
INPUT_PATH = "/content/base_datos_completa_NNA_TI_anon.xlsx"

SHEET_NAME = "BD"   # Hoja a procesar
OUT_DIR = "/content/data/procesados"
REP_DIR = "/content/reports"
os.makedirs(OUT_DIR, exist_ok=True)
os.makedirs(REP_DIR, exist_ok=True)

# Variables objetivo (solo trabajamos con estas)
RAW_TARGET_COLUMNS = [
    "AFILIACI√ìN AL SGSSS",                 # ‚úÖ reemplaza a Personas_a_Cargo
    "CATEGOR√çAS_DE_LA_DISCAPACIDAD",
    "Localidad_fic",
    "NACIONALIDAD",
    "SEXO",
    "¬øEN_DONDE_REALIZA_PRINCIPALMENTE_SU_TRABAJO?",
    "V√çNCULO_CON_EL_JEFE_DE_HOGAR",
    "EDAD",
]

CAT_IMPUTE_VALUE = "No especificado"
print("Ruta usada:", INPUT_PATH)

# ==== CELDA 3: UTILIDADES ====

# Aliases para AFILIACI√ìN AL SGSSS (por si el nombre cambia en el Excel)
SGSSS_ALIASES = [
    "afiliacion_al_sgsss", "afiliacion_sgsss", "afiliaci√≥n_al_sgsss", "afiliacion_sgss",
    "regimen_de_afiliacion", "regimen_salud", "regimen", "tipo_de_afiliacion", "tipo_afiliacion",
]

def strip_accents(s: str) -> str:
    return "".join(ch for ch in unicodedata.normalize("NFKD", str(s)) if not unicodedata.combining(ch))

def slug(s: str) -> str:
    s = strip_accents(str(s)).lower().strip()
    s = re.sub(r"[^\w]+", "_", s)
    s = re.sub(r"_+", "_", s).strip("_")
    return s

def slug_map(cols: List[str]) -> Dict[str, str]:
    m = {}
    for c in cols:
        sc = slug(c)
        i, base = 2, sc
        while sc in m and m[sc] != c:
            sc = f"{base}_{i}"; i += 1
        m[sc] = c
    return m

def summarize_missing(df: pd.DataFrame) -> pd.DataFrame:
    return (pd.DataFrame({
        "Variable": df.columns,
        "Faltantes": df.isna().sum().values,
        "Porcentaje": (df.isna().mean().values * 100).round(2),
        "Tipo": [str(t) for t in df.dtypes.values],
        "Valores_Unicos": [df[c].nunique(dropna=True) for c in df.columns],
    }).sort_values("Porcentaje", ascending=False))

# Sentinel robusto: 9s repetidos (‚â•4) ‚Üí NaN (cubre 9999, 99999, 9,999, 99999.0, -9999, etc.)
def is_sentinel_9(val, min_nines: int = 4) -> bool:
    if pd.isna(val): return False
    s = str(val).strip().replace(",", "")
    if re.fullmatch(r"-?\d+(?:\.\d+)?", s):
        try:
            v = float(s)
            if v.is_integer():
                s_int = str(int(abs(v)))
                return len(s_int) >= min_nines and set(s_int) == {"9"}
        except Exception:
            return False
    s_digits = re.sub(r"[^0-9]", "", s)
    return len(s_digits) >= min_nines and set(s_digits) == {"9"}

# Localidad: corrige mojibake y deja SOLO MAY√öSCULAS (sin s√≠mbolos raros)
def normalize_localidad(s: str) -> str:
    if pd.isna(s): return np.nan
    s = strip_accents(str(s)).strip()
    s = re.sub(r"[^A-Za-z\s]", "", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s.upper()

# Normalizador de categor√≠as de AFILIACI√ìN AL SGSSS
def normalize_sgsss(s: str) -> str:
    """Mapea a categor√≠as est√°ndar: CONTRIBUTIVO, SUBSIDIADO, ESPECIAL, NO AFILIADO, BENEFICIARIO, VINCULADO, OTRO"""
    if pd.isna(s): return np.nan
    t = strip_accents(str(s)).upper().strip()
    t = re.sub(r"\s+", " ", t)

    # patrones
    if re.search(r"CONTRIBUT", t):   return "CONTRIBUTIVO"
    if re.search(r"SUBSIDI", t):     return "SUBSIDIADO"
    if re.search(r"ESPECIAL|EXCEPC", t): return "ESPECIAL/EXCEPCION"
    if re.search(r"NO\s*AFILI", t):  return "NO AFILIADO"
    if re.search(r"BENEFIC", t):     return "BENEFICIARIO"
    if re.search(r"VINCUL", t):      return "VINCULADO"
    if re.search(r"REGIMEN|REGIM√âN|REGIMN", t):  # cualquier ‚Äúregimen‚Äù sin palabra clave clara
        return t
    return t  # por defecto devuelve la versi√≥n limpia en MAY√öSCULAS

# ==== CELDA 4: LIMPIEZA PRINCIPAL ====

# 1) Leer hoja BD
df_raw = pd.read_excel(INPUT_PATH, sheet_name=SHEET_NAME, engine="openpyxl")
print(f"Le√≠das {df_raw.shape[0]:,} filas √ó {df_raw.shape[1]} columnas de '{SHEET_NAME}'")

# 2) Resolver nombres por slug
col_slug_map = slug_map(list(df_raw.columns))
available_slugs = set(col_slug_map.keys())

targets_resolved: Dict[str, Optional[str]] = {}
for wanted in RAW_TARGET_COLUMNS:
    w_slug = slug(wanted)
    found = None

    if w_slug in available_slugs:
        found = col_slug_map[w_slug]
    else:
        # alias/heur√≠stica para AFILIACI√ìN AL SGSSS
        if w_slug in {"afiliacion_al_sgsss"}:
            for alias in SGSSS_ALIASES:
                if alias in available_slugs:
                    found = col_slug_map[alias]; break
            if not found:
                # intento por tokens gen√©ricos
                for s in available_slugs:
                    if ("sgsss" in s) or ("afiliacion" in s) or ("regimen" in s) or ("salud" in s):
                        found = col_slug_map[s]; break
        # patr√≥n general por tokens
        if not found:
            tokens = [t for t in w_slug.split("_") if t not in {"de","del","la","el","los","las","y","con","su","en","al"}]
            pattern = re.compile("|".join(map(re.escape, tokens)), flags=re.I) if tokens else None
            if pattern:
                for s in available_slugs:
                    if pattern.search(s):
                        found = col_slug_map[s]; break
    targets_resolved[wanted] = found

print("\nMapeo columnas objetivo:")
for k, v in targets_resolved.items():
    print(f"  - {k} -> {v if v else 'NO ENCONTRADA'}")

present_cols = [v for v in targets_resolved.values() if v]
if not present_cols:
    raise SystemExit("‚ùå No se encontraron columnas objetivo. Revisa nombres en la hoja BD.")

# 3) Subset solo con las columnas encontradas
df = df_raw[present_cols].copy()

# 4) Limpieza b√°sica de texto
for c in df.select_dtypes(include=["object"]).columns:
    df[c] = df[c].apply(lambda x: re.sub(r"\s+", " ", str(x)).strip() if pd.notna(x) else x)

# 5) Reemplazo de sentinelas 9s ‚Üí NaN
df = df.mask(df.applymap(lambda x: is_sentinel_9(x, min_nines=4)), np.nan)

# 6) Localidades en MAY√öSCULAS limpias
col_localidad = targets_resolved.get("Localidad_fic")
if col_localidad and col_localidad in df.columns:
    df[col_localidad] = df[col_localidad].apply(normalize_localidad)

# 7) AFILIACI√ìN AL SGSSS ‚Üí normalizar categor√≠as en MAY√öSCULAS limpias
col_sgsss = targets_resolved.get("AFILIACI√ìN AL SGSSS")
if col_sgsss and col_sgsss in df.columns:
    df.rename(columns={col_sgsss: "AFILIACION_AL_SGSSS"}, inplace=True)  # nombre est√°ndar
    df["AFILIACION_AL_SGSSS"] = df["AFILIACION_AL_SGSSS"].apply(normalize_sgsss)

# 8) Quitar EDAD = 0.0 (forzar num√©rico)
col_edad = targets_resolved.get("EDAD")
if col_edad and col_edad in df.columns:
    df.rename(columns={col_edad: "EDAD"}, inplace=True)
    df["EDAD"] = pd.to_numeric(df["EDAD"], errors="coerce")
    antes = len(df)
    df = df[df["EDAD"].notna() & (df["EDAD"] != 0.0)]
    print(f"Eliminadas {antes - len(df):,} filas con EDAD = 0.0")

# 9) Imputaci√≥n: num√©ricas‚Üímediana; categ√≥ricas‚Üí'No especificado'
num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
cat_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
impute_summary: Dict[str, Any] = {"numeric": {}, "categorical": {}}

for c in num_cols:
    if df[c].isna().any():
        med = df[c].median()
        df[c].fillna(med, inplace=True)
        impute_summary["numeric"][c] = None if pd.isna(med) else float(med)

for c in cat_cols:
    if df[c].isna().any():
        df[c].fillna(CAT_IMPUTE_VALUE, inplace=True)
        impute_summary["categorical"][c] = CAT_IMPUTE_VALUE

print("\n‚úÖ Limpieza terminada")
df.head()

"""## Objetivo II ##
Analizar las condiciones familiares y de acompa√±amiento social que se reportan en los registros, con el fin de entender mejor el entorno en que se presenta el trabajo infantil.
"""

import pandas as pd, numpy as np
import matplotlib.pyplot as plt
from math import sqrt
from scipy.stats import chi2_contingency

up = files.upload()  # selecciona "DatasetConVariables elegidas.xlsx"
FILE = next(iter(up))
df = pd.read_excel(FILE, dtype="string")
df.shape, df.columns.tolist()

col_lugar = '¬øEN DONDE REALIZA PRINCIPALMENTE SU TRABAJO?'
col_sgsss = 'AFILIACION_AL_SGSSS'
col_vinc  = 'V√çNCULO CON EL JEFE DE HOGAR'

# chequeo r√°pido
for c in [col_lugar, col_sgsss, col_vinc]:
    print(c, '‚Üí n=', df[c].notna().sum(), 'categor√≠as=', df[c].nunique(dropna=True))

vc = df[col_lugar].value_counts().sort_values(ascending=False)
labels = vc.index.astype(str)
vals = (100*vc/vc.sum()).values

plt.figure(figsize=(8,4))
bars = plt.bar(labels, vals)
plt.xticks(rotation=45, ha='right'); plt.ylabel('%')
plt.title('¬øD√≥nde se presenta el trabajo infantil?')

for b, v in zip(bars, vals):
    plt.text(b.get_x()+b.get_width()/2, b.get_height(), f'{v:.1f}%', ha='center', va='bottom', fontsize=9)

plt.ylim(0, max(vals)*1.15); plt.tight_layout(); plt.show()

"""la ocurrencia se concentra en una sola ubicaci√≥n. ‚ÄúEn una UTI‚Äù aporta 60.6% de los registros, seguida por ‚Äúen la calle, estacionario o ambulante‚Äù 15.8% y ‚ÄúNo especificado‚Äù 15.6%; ‚Äúen la vivienda que habita‚Äù 7.8% y el resto de lugares son marginales (~0‚Äì0.1%). Implicaci√≥n: prioriza acciones donde est√° el volumen (UTI) y un segundo frente en calle/ambulancia; el 15.6% no especificado limita comparaciones y debe depurarse; la fracci√≥n en vivienda sugiere intervenci√≥n familiar y visitas domiciliarias."""

def barras_top3(var, col_lugar, titulo=None):
    ct = pd.crosstab(df[var], df[col_lugar])
    # top-3 lugares globales, excluye "No especificado" si existe
    cols_ord = ct.sum().sort_values(ascending=False).index.tolist()
    cols_ord = [c for c in cols_ord if str(c).strip().lower()!='no especificado']
    top3 = cols_ord[:3]
    otros = [c for c in ct.columns if c not in top3]
    ct2 = pd.DataFrame(index=ct.index)
    for c in top3: ct2[c] = ct[c]
    ct2['Otros'] = ct[otros].sum(axis=1)
    pct = (ct2.div(ct2.sum(axis=1), axis=0)*100).fillna(0)
    # ordena filas por el primer lugar Top
    pct = pct.sort_values(top3[0], ascending=False)

    x = np.arange(len(pct))
    w = 0.22
    plt.figure(figsize=(9,4))
    for i,col in enumerate(pct.columns):
        plt.bar(x + (i-1.5)*w, pct[col].values, width=w, label=col)
        for j,val in enumerate(pct[col].values):
            if val>=5:
                plt.text(x[j] + (i-1.5)*w, val+1, f'{val:.0f}%', ha='center', va='bottom', fontsize=8)

    plt.xticks(x, pct.index.astype(str), rotation=45, ha='right')
    plt.ylabel('%')
    plt.ylim(0, 100)
    plt.title(titulo or f'{var}: composici√≥n por lugar (Top-3 + Otros)')
    plt.legend(ncol=4, fontsize=9)
    plt.tight_layout(); plt.show()

    return pct.reset_index()

pct_sgsss = barras_top3(col_sgsss, col_lugar, 'AFILIACION_AL_SGSSS √ó lugar de trabajo')

"""UTI es el principal entorno en todas las afiliaciones: Contributivo 71%, Especial/Excepci√≥n 71%, Vinculado 60%, No especificado 58%, Subsidiado 58% y Sin aseguramiento 45%. La calle crece sobre todo en Vinculado 40% y Sin aseguramiento 33%; es menor en Subsidiado 16% y Contributivo ~9%. El trabajo en vivienda es bajo en todos (0‚Äì11%). La categor√≠a ‚ÄúNo especificado‚Äù mezcla entornos y concentra 29% en ‚ÄúOtros‚Äù, por lo que reduce precisi√≥n. Conclusi√≥n operativa: el foco general es UTI; refuerza intervenci√≥n en calle para Vinculado y No asegurado."""

import numpy as np, pandas as pd, matplotlib.pyplot as plt

var = col_vinc
ct = pd.crosstab(df[var], df[col_lugar])

# Top-3 lugares globales (excluye "No especificado"); resto ‚Üí "Otros"
cols_ord = ct.sum().sort_values(ascending=False).index.tolist()
cols_ord = [c for c in cols_ord if str(c).strip().lower()!='no especificado']
top3 = cols_ord[:3]; otros = [c for c in ct.columns if c not in top3]
ct2 = pd.DataFrame(index=ct.index)
for c in top3: ct2[c] = ct[c]
ct2['Otros'] = ct[otros].sum(axis=1)

# % por fila y orden por %UTI si existe
pct = (ct2.div(ct2.sum(axis=1), axis=0)*100).fillna(0)
col_uti = next((c for c in pct.columns if 'uti' in c.lower()), pct.columns[0])
pct = pct.sort_values(col_uti, ascending=False)

# plot 100% stacked horizontal
plt.figure(figsize=(9, 0.45*len(pct)+1))
left = np.zeros(len(pct))
for col in pct.columns:
    plt.barh(pct.index.astype(str), pct[col].values, left=left, label=col)
    # anota solo segmentos ‚â•8%
    for i, v in enumerate(pct[col].values):
        if v >= 8:
            plt.text(left[i]+v/2, i, f'{v:.0f}%', va='center', ha='center', fontsize=8)
    left += pct[col].values
plt.xlabel('%'); plt.title('V√çNCULO √ó lugar de trabajo ‚Äî composici√≥n (Top-3 + Otros)')
plt.legend(ncol=4, fontsize=9); plt.xlim(0,100); plt.tight_layout(); plt.show()

"""el entorno dominante es UTI para relaciones nucleares con el jefe del hogar (‚âàJefe 84% UTI, Familia 68%, Hermano(a) 67%, Hijo(a) 61%). La calle se dispara en v√≠nculos at√≠picos: Padre/Madre 82% calle y Nuera/Yerno 50% calle; tambi√©n sube en Nieto(a) ~23% y C√≥nyuge ~19%. Los ‚Äúno parientes‚Äù y Abuelo(a) concentran ‚ÄúOtros‚Äù (‚âà53% y 58%). Vivienda es minoritaria en todos (‚â≤20%), con v√≠nculos nucleares el trabajo ocurre sobre todo en UTI; con v√≠nculos no filiales o asim√©tricos aumenta la exposici√≥n en calle. Prioriza intervenci√≥n en calle para Padre/Madre y Nuera/Yerno y mant√©n acciones transversales en UTI."""

def top10_combos(var, col_lugar):
    ct = pd.crosstab(df[var], df[col_lugar])
    tall = ct.stack().reset_index()
    tall.columns = [var, 'lugar', 'n']
    denom = ct.sum(axis=1).rename('n_categoria').reset_index()
    out = tall.merge(denom, on=var, how='left')
    out['% dentro de la categor√≠a'] = 100*out['n']/out['n_categoria']
    out['% del total'] = 100*out['n']/len(df)
    out = out.sort_values('n', ascending=False).head(10)
    return out

top_sgsss = top10_combos(col_sgsss, col_lugar)
top_vinc  = top10_combos(col_vinc,  col_lugar)

# ver
top_sgsss, top_vinc

"""Acompa√±amiento (SGSSS). El volumen se concentra en UTI:

 * Subsidado‚ÄìUTI: 7 465 casos (58.2% dentro de subsidado, 25.1% del total).

* Contributivo‚ÄìUTI: 6 382 (71.5% dentro de contributivo, 21.5% del total).

* La calle pesa cuando la cobertura es d√©bil: No asegurado‚Äìcalle 1 336 (32.6% de no asegurado, 4.5% del total) y No asegurado‚ÄìUTI 1 850 (45.2%, 6.2% del total).

* En subsidado la calle existe pero menor: Subsidado‚Äìcalle 2 368 (18.5% de subsidado, 8.0% del total).

* Vivienda es marginal: Subsidado‚Äìvivienda 1 003 (7.8%, 3.37% del total).

* ‚ÄúNo especificado‚Äù es relevante (p.ej., No especificado‚ÄìUTI 2 022, 6.8% del total) y reduce precisi√≥n.

Condici√≥n familiar (v√≠nculo con el jefe). El fen√≥meno es principalmente filial y en UTI:

* Hijo(a)‚ÄìUTI: 16 286 (61.4% dentro de hijo[a], 54.8% del total).

* Hijo(a)‚Äìcalle: 4 169 (15.7%, 14.0% del total).

* Hijo(a)‚Äìvivienda: 2 159 (8.1%, 7.26% del total).

* Otros v√≠nculos aportan poco al total: Nieto(a)‚ÄìUTI 605 (2.04% del total), Otro pariente‚ÄìUTI 404 (1.36%), Nieto(a)‚Äìcalle 239 (0.80%).

* No parientes/otros concentran ‚ÄúOtros/No especificado‚Äù, no la calle ni la vivienda.

Las condiciones familiares muestran que el trabajo infantil registrado ocurre mayoritariamente con hijos(as) del jefe y en UTI; las diferencias de acompa√±amiento indican que la exposici√≥n en calle aumenta cuando no hay aseguramiento y existe, en menor medida, aun con subsidio. Prioriza intervenci√≥n transversal en UTI, y refuerza acciones en calle para no asegurados; mant√©n el enfoque familiar en hogares con hijos(as) del jefe y mejora el registro de ‚ÄúNo especificado‚Äù para afinar el diagn√≥stico.

## Objetivo III
Identificar diferencias relevantes entre localidades que permitan dimensionar c√≥mo var√≠a el fen√≥meno dentro de la ciudad.
"""

df = pd.read_excel("/content/DatasetConVariables elegidas.xlsx")
df.columns = df.columns.str.strip().str.upper()
col_loc = "LOCALIDAD_FIC"
col_edad = "EDAD"
col_sexo = "SEXO"
col_nac  = "NACIONALIDAD"

df[col_edad] = pd.to_numeric(df[col_edad], errors='coerce')
ninos = df[df[col_edad] < 18].copy()
ninos[col_loc] = ninos[col_loc].astype(str).str.strip().str.upper()

print(f"‚úÖ Total de registros: {len(df)}")
print(f"‚úÖ Ni√±os (<18 a√±os): {len(ninos)}\n")

# ==============================================================
# üî∏ 1. Distribuci√≥n de ni√±os por localidad
# ==============================================================
conteo_localidad = ninos[col_loc].value_counts().sort_values(ascending=False)

plt.figure(figsize=(10,6))
sns.barplot(x=conteo_localidad.values, y=conteo_localidad.index, palette="YlOrRd")
plt.title("N√∫mero de ni√±os por localidad")
plt.xlabel("Cantidad de ni√±os")
plt.ylabel("Localidad")
plt.tight_layout()
fig1 = plt.gcf()
plt.show()

print("\nDistribuci√≥n de ni√±os por localidad:")
display(conteo_localidad)

# ==============================================================
# üî∏ 2. Cruce Localidad √ó Sexo
# ==============================================================
ninos[col_sexo] = ninos[col_sexo].astype(str).str.strip().str.upper()
tabla_sexo = pd.crosstab(ninos[col_loc], ninos[col_sexo], normalize="index") * 100

plt.figure(figsize=(10,6))
sns.heatmap(tabla_sexo, annot=True, fmt=".1f", cmap="coolwarm")
plt.title("% por sexo dentro de cada localidad")
plt.xlabel("Sexo")
plt.ylabel("Localidad")
plt.tight_layout()
fig2 = plt.gcf()
plt.show()

print("\nPorcentaje por sexo dentro de cada localidad:")
display(tabla_sexo.round(1))

# ==============================================================
# üî∏ 3. Cruce Localidad √ó Nacionalidad
# ==============================================================
ninos[col_nac] = ninos[col_nac].astype(str).str.strip().str.upper()
ninos["NAC_SIMPLE"] = ninos[col_nac].apply(lambda x: "COLOMBIANA" if "COL" in x else "EXTRANJERA")

tabla_nac = pd.crosstab(ninos[col_loc], ninos["NAC_SIMPLE"], normalize="index") * 100

plt.figure(figsize=(10,6))
sns.heatmap(tabla_nac, annot=True, fmt=".1f", cmap="YlGnBu")
plt.title("% de nacionalidad por localidad")
plt.xlabel("Nacionalidad")
plt.ylabel("Localidad")
plt.tight_layout()
fig3 = plt.gcf()
plt.show()

print("\nPorcentaje de nacionalidad por localidad:")
display(tabla_nac.round(1))

# ==============================================================
# üìÅ EXPORTAR TODO EN UN SOLO BLOQUE FINAL
# ==============================================================
os.makedirs("reports", exist_ok=True)

# Guardar tablas
conteo_localidad.to_csv("reports/ninos_por_localidad.csv", header=["Cantidad"], encoding="utf-8")
tabla_sexo.round(1).to_csv("reports/sexo_por_localidad.csv", encoding="utf-8")
tabla_nac.round(1).to_csv("reports/nacionalidad_por_localidad.csv", encoding="utf-8")

# Guardar figuras
fig1.savefig("reports/ninos_por_localidad.png", dpi=300, bbox_inches='tight')
fig2.savefig("reports/sexo_por_localidad.png", dpi=300, bbox_inches='tight')
fig3.savefig("reports/nacionalidad_por_localidad.png", dpi=300, bbox_inches='tight')

print("\n‚úÖ Todas las tablas y gr√°ficos fueron exportados correctamente a la carpeta 'reports/'")